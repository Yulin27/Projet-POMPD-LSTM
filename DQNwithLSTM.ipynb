{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7hnsNizDnMw",
        "outputId": "3fb95fa4-d8bd-4591-c58a-164b75850707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.8/dist-packages (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata==4.13.0) (3.13.0)\n",
            "ERROR: unknown command \"in!pip\"\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.8/dist-packages (2.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf) (6.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.8/dist-packages (from omegaconf) (4.9.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl_gym.git\n",
            "  Cloning https://github.com/osigaud/bbrl_gym.git to /tmp/pip-req-build-2txvpt3j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl_gym.git /tmp/pip-req-build-2txvpt3j\n",
            "  Resolved https://github.com/osigaud/bbrl_gym.git to commit 5557075ecd7d4171ac0c21be3c69a94bcae655a9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (4.1.1)\n",
            "Requirement already satisfied: mazemdp>=0.7.3 in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (0.7.3)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (1.21.6)\n",
            "Requirement already satisfied: gym==0.21.0 in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (0.21.0)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (2.3.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21.0->bbrl-gym==1.2.5) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl.git\n",
            "  Cloning https://github.com/osigaud/bbrl.git to /tmp/pip-req-build-e7l92nkw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl.git /tmp/pip-req-build-e7l92nkw\n",
            "  Resolved https://github.com/osigaud/bbrl.git to commit 5e8530b3eba8b14084deafe83795b93afca618f9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install importlib-metadata==4.13.0\n",
        "!pip in!pip install importlib-metadata==4.13.0stall importlib-metadata==4.13.0\n",
        "import os\n",
        "import functools\n",
        "import time\n",
        "!pip install omegaconf\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import gym\n",
        "!pip install git+https://github.com/osigaud/bbrl_gym.git\n",
        "!pip install git+https://github.com/osigaud/bbrl.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bbrl\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nTocnUeRMp2j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bbrl.agents.agent import Agent\n",
        "from bbrl import get_arguments, get_class, instantiate_class\n",
        "\n",
        "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
        "from bbrl.workspace import Workspace\n",
        "\n",
        "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
        "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
        "# or until a given condition is reached\n",
        "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
        "\n",
        "#Replay Buffer\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n",
        "# with auto-resetting. These agents produce multiple variables in the workspace: \n",
        "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n",
        "# ... When called at timestep t=0, then the environments are automatically reset. \n",
        "# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n",
        "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
        "# Not present in the A2C version...\n",
        "from bbrl.utils.logger import TFLogger"
      ],
      "metadata": {
        "id": "SzAEphDJDzsk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    print(sizes)\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers = [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "IHNLO69nD3f_"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = build_mlp(\n",
        "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        #print(obs)\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(1)\n",
        "            self.set((\"action\", t), action) "
      ],
      "metadata": {
        "id": "TaoZg3-DD4xq"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_dim, nb_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(state_dim, hidden_dim, nb_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
        "        self.nb_layers = nb_layers\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        # print(obs)\n",
        "        batch_size = obs.shape[0]\n",
        "        h0 = torch.zeros(self.nb_layers, batch_size, self.hidden_dim).to(obs.device)\n",
        "        c0 = torch.zeros(self.nb_layers, batch_size, self.hidden_dim).to(obs.device)\n",
        "        # print(obs.shape, h0.shape,c0.shape)\n",
        "\n",
        "        lstm_out, _ = self.lstm(obs.unsqueeze(0), (h0, c0))  \n",
        "        q_values = self.fc(lstm_out[:, -1, :])\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(1)\n",
        "            self.set((\"action\", t), action)\n"
      ],
      "metadata": {
        "id": "E3Ih9vH7QlxO"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_name):\n",
        "    return gym.make(env_name)"
      ],
      "metadata": {
        "id": "NPuyITmnD6ar"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EGreedyActionSelector(Agent):\n",
        "    def __init__(self, epsilon):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        q_values = self.get((\"q_values\", t))\n",
        "        nb_actions = q_values.size()[1]\n",
        "        size = q_values.size()[0]\n",
        "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
        "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
        "        max_action = q_values.max(1)[1]\n",
        "        action = is_random * random_action + (1 - is_random) * max_action\n",
        "        action = action.long()\n",
        "        self.set((\"action\", t), action)"
      ],
      "metadata": {
        "id": "MX7ccRhRD7yC"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_env_agents(cfg):\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "    get_class(cfg.gym_env),\n",
        "    get_arguments(cfg.gym_env),\n",
        "    cfg.algorithm.nb_evals,\n",
        "    cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent"
      ],
      "metadata": {
        "id": "2st9rCPBD9pY"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "    #critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
        "    critic = LSTMQAgent(obs_size, cfg.algorithm.architecture.hidden_size, 2, act_size)\n",
        "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
        "    q_agent = TemporalAgent(critic)\n",
        "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
        "    ev_agent = Agents(eval_env_agent, critic)\n",
        "\n",
        "    # Get an agent that is executed on a complete workspace\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "    return train_agent, eval_agent, q_agent\n"
      ],
      "metadata": {
        "id": "NqikpWm0KK8I"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger():\n",
        "\n",
        "  def __init__(self, cfg):\n",
        "    self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "  def add_log(self, log_string, loss, epoch):\n",
        "    self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "  # Log losses\n",
        "  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n",
        "    self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "    self.add_log(\"actor_loss\", a2c_loss, epoch)\n"
      ],
      "metadata": {
        "id": "g-Smn_58EAn_"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the optimizer over the q agent\n",
        "def setup_optimizers(cfg, q_agent):\n",
        "    optimizer_args = get_arguments(cfg.optimizer)\n",
        "    parameters = q_agent.parameters()\n",
        "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "8Z9Run9zECAe"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, action):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        cfg (_type_): _description_\n",
        "        reward (torch.Tensor): A (T x B) tensor containing the rewards\n",
        "        must_bootstrap (torch.Tensor): a (T x B) tensor containing 0 if the episode is completed at time $t$\n",
        "        q_values (torch.Tensor): a (T x B x A) tensor containing Q values\n",
        "        action (torch.LongTensor): a (T) long tensor containing the chosen action\n",
        "\n",
        "    Returns:\n",
        "        torch.Scalar: The DQN loss and the temporal difference\n",
        "    \"\"\"\n",
        "    # We compute the max of Q-values over all actions\n",
        "    max_q = q_values.max(-1)[0].detach()\n",
        "\n",
        "    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n",
        "    # The same about must_bootstrap. \n",
        "    target = (\n",
        "        reward[:-1] + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
        "    )\n",
        "    # To get Q(s,a), we use torch.gather along the 3rd dimension (the action)\n",
        "    act = action[0].unsqueeze(-1)\n",
        "    qvals = torch.gather(q_values[0], dim=1, index=act).squeeze()\n",
        "\n",
        "    # Compute the temporal difference (use must_boostrap as to mask out finished episodes)\n",
        "    td = target - qvals \n",
        "    # Compute critic loss\n",
        "    td_error = td**2\n",
        "    critic_loss = td_error.mean()\n",
        "    return critic_loss, td\n",
        "    "
      ],
      "metadata": {
        "id": "qcdYS_P6EDna"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn(cfg, max_grad_norm=0.5):\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg)\n",
        "    best_reward = -10e9\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
        "\n",
        "    # 3) Create the DQN-like Agent\n",
        "    train_agent, eval_agent, q_agent = create_dqn_agent(\n",
        "        cfg, train_env_agent, eval_env_agent\n",
        "    )\n",
        "\n",
        "    # 5) Configure the workspace to the right dimension\n",
        "    # Note that no parameter is needed to create the workspace.\n",
        "    # In the training loop, calling the agent() and critic_agent()\n",
        "    # will take the workspace as parameter\n",
        "    train_workspace = Workspace()  # Used for training\n",
        "\n",
        "    # 6) Configure the optimizer over the a2c agent\n",
        "    optimizer = setup_optimizers(cfg, q_agent)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "\n",
        "    # 7) Training loop\n",
        "    for epoch in range(cfg.algorithm.max_epochs):\n",
        "        print(epoch)\n",
        "        # Execute the agent in the workspace\n",
        "        if epoch > 0:\n",
        "            train_workspace.zero_grad()\n",
        "            train_workspace.copy_n_last_steps(1)\n",
        "            train_agent(\n",
        "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n",
        "            )\n",
        "        else:\n",
        "            train_agent(\n",
        "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
        "            )\n",
        "\n",
        "        transition_workspace = train_workspace.get_transitions()\n",
        "\n",
        "        # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace).\n",
        "        \n",
        "\n",
        "        q_values, done, truncated, reward, action = transition_workspace[\n",
        "            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
        "        ]\n",
        "\n",
        "        nb_steps += len(action[0]) * cfg.algorithm.n_envs\n",
        "\n",
        "        # Determines whether values of the critic should be propagated\n",
        "        # True if the episode reached a time limit or if the task was not done\n",
        "        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
        "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "        # Compute critic loss\n",
        "        critic_loss, td = compute_critic_loss(\n",
        "            cfg, reward, must_bootstrap, q_values, action\n",
        "        )\n",
        "\n",
        "        # Store the loss for tensorboard display\n",
        "        logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(q_agent.parameters(), max_grad_norm)\n",
        "        obs = train_workspace[\"env/env_obs\"].shape\n",
        "        # print(obs)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "            tmp_steps = nb_steps\n",
        "\n",
        "            eval_workspace = Workspace()  # Used for evaluation\n",
        "\n",
        "            eval_agent(\n",
        "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
        "            )\n",
        "\n",
        "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "\n",
        "            mean = rewards.mean()\n",
        "            logger.add_log(\"reward\", mean, nb_steps)\n",
        "            print(f\"epoch: {epoch}, reward: {mean}\")\n",
        "            if cfg.save_best and mean > best_reward:\n",
        "                best_reward = mean\n",
        "                directory = \"./dqn0_critic/\"\n",
        "                if not os.path.exists(directory):\n",
        "                    os.makedirs(directory)\n",
        "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n",
        "                eval_agent.save_model(filename)\n"
      ],
      "metadata": {
        "id": "ZCbMYucaEF-l"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={\n",
        "  \"save_best\": False,\n",
        "  \"logger\":{\n",
        "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "    \"log_dir\": \"./tmp/\" + str(time.time()),\n",
        "    \"cache_size\": 10000,\n",
        "    \"every_n_seconds\": 10,\n",
        "    \"verbose\": False,    \n",
        "    },\n",
        "\n",
        "  \"algorithm\":{\n",
        "    \"seed\": 5,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"n_envs\": 1,\n",
        "    \"n_steps\": 100,\n",
        "    \"eval_interval\": 2000,\n",
        "    \"nb_evals\": 10,\n",
        "    \"gae\": 0.8,\n",
        "    \"max_epochs\": 3500,\n",
        "    \"discount_factor\": 0.99,\n",
        "    \"batch_size\": 256,\n",
        "    \"max_buffer_size\": 1e5,\n",
        "    \"target_critic_update\": 5000,\n",
        "    #\"architecture\":{\"hidden_size\": [256,256]},\n",
        "    \"architecture\":{\"hidden_size\": 256, \"nb_layers\":2},\n",
        "  },\n",
        "  \"gym_env\":{\n",
        "    \"classname\": \"__main__.make_env\",\n",
        "    \"env_name\": \"CartPole-v1\",\n",
        "  },\n",
        "  \"optimizer\":        # print(batch_size)\n",
        "\n",
        "  {\n",
        "    \"classname\": \"torch.optim.Adam\",\n",
        "    \"lr\": 2e-3,\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "RKSIpUQeEIYd"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=OmegaConf.create(params)\n",
        "torch.manual_seed(config.algorithm.seed)\n",
        "run_dqn(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "97XmFJ8NEMMh",
        "outputId": "0cc95478-72a6-44ca-e459-cf484088e0bd"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-278-776ecb2c75e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-274-c5ba21e490b9>\u001b[0m in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, max_grad_norm)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0meval_workspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Used for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             eval_agent(\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0meval_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"env/done\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0m_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_variable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mworkspace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[Agent.__call__] workspace must not be None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-277-9c55db4108a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, choose_action, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# print(obs.shape, h0.shape,c0.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q_values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0;31m# the user believes he/she is passing in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    696\u001b[0m                            ):\n\u001b[1;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    699\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    700\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    229\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 1, 256), got [2, 10, 256]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./tmp"
      ],
      "metadata": {
        "id": "yBTXX7_hEKTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3X19Hz_tVtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}