{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M7hnsNizDnMw",
        "outputId": "2c432ac0-8f7b-491b-f043-d2494def2cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting importlib-metadata==4.13.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata==4.13.0) (3.13.0)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "Successfully installed importlib-metadata-4.13.0\n",
            "ERROR: unknown command \"in!pip\"\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=d22f09a2541bd287221339c04a7941e4614c8eccd5e0652697fef778df0a97a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl_gym.git\n",
            "  Cloning https://github.com/osigaud/bbrl_gym.git to /tmp/pip-req-build-rrffejis\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl_gym.git /tmp/pip-req-build-rrffejis\n",
            "  Resolved https://github.com/osigaud/bbrl_gym.git to commit 5557075ecd7d4171ac0c21be3c69a94bcae655a9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from bbrl-gym==1.2.5) (1.21.6)\n",
            "Collecting mazemdp>=0.7.3\n",
            "  Downloading mazemdp-0.7.3-py3-none-any.whl (15 kB)\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21.0->bbrl-gym==1.2.5) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mazemdp>=0.7.3->bbrl-gym==1.2.5) (1.15.0)\n",
            "Building wheels for collected packages: bbrl-gym, gym, box2d-py\n",
            "  Building wheel for bbrl-gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bbrl-gym: filename=bbrl_gym-1.2.5-py3-none-any.whl size=17599 sha256=0cd769c90036ade960692a8cc5b52fe1258405f1acf0ceec3b58c06756033647\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c8f4v9c8/wheels/bf/5a/48/c295a922463ac7a2fd84f8ac70900ef8ce394d7f8b161d945f\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616825 sha256=2ef7dd0503aae3fef88f70abb4962e4e0226a65bd7373dff81a71e5d86f84ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Successfully built bbrl-gym gym\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, gym, mazemdp, bbrl-gym\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed bbrl-gym-1.2.5 box2d-py-2.3.8 gym-0.21.0 mazemdp-0.7.3 swig-4.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/osigaud/bbrl.git\n",
            "  Cloning https://github.com/osigaud/bbrl.git to /tmp/pip-req-build-s3ld0m6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl.git /tmp/pip-req-build-s3ld0m6q\n",
            "  Resolved https://github.com/osigaud/bbrl.git to commit 5e8530b3eba8b14084deafe83795b93afca618f9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: bbrl\n",
            "  Building wheel for bbrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bbrl: filename=bbrl-0.1.9-py3-none-any.whl size=50513 sha256=689555ece9e928e8c370289f822c5082bcc82fae03f413db3af213f00ae79e8b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1n7m9y4c/wheels/cc/1d/b5/74c1c5cbfde8f8270cbcfa773985a95000d73eb26c771c7eff\n",
            "Successfully built bbrl\n",
            "Installing collected packages: bbrl\n",
            "Successfully installed bbrl-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install importlib-metadata==4.13.0\n",
        "!pip in!pip install importlib-metadata==4.13.0stall importlib-metadata==4.13.0\n",
        "import os\n",
        "import functools\n",
        "import time\n",
        "!pip install omegaconf\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import gym\n",
        "!pip install git+https://github.com/osigaud/bbrl_gym.git\n",
        "!pip install git+https://github.com/osigaud/bbrl.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bbrl\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nTocnUeRMp2j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bbrl.agents.agent import Agent\n",
        "from bbrl import get_arguments, get_class, instantiate_class\n",
        "\n",
        "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
        "from bbrl.workspace import Workspace\n",
        "\n",
        "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
        "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
        "# or until a given condition is reached\n",
        "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
        "\n",
        "#Replay Buffer\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n",
        "# with auto-resetting. These agents produce multiple variables in the workspace: \n",
        "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n",
        "# ... When called at timestep t=0, then the environments are automatically reset. \n",
        "# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n",
        "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
        "# Not present in the A2C version...\n",
        "from bbrl.utils.logger import TFLogger"
      ],
      "metadata": {
        "id": "SzAEphDJDzsk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    print(sizes)\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers = [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "IHNLO69nD3f_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = build_mlp(\n",
        "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        #print(obs)\n",
        "        q_values = self.model(obs).squeeze(-1)\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(1)\n",
        "            self.set((\"action\", t), action) "
      ],
      "metadata": {
        "id": "TaoZg3-DD4xq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_dim, nb_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(state_dim, hidden_dim, nb_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
        "        self.nb_layers = nb_layers\n",
        "\n",
        "    def forward(self, t, choose_action=True, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        # print(obs)\n",
        "        batch_size = obs.shape[0]\n",
        "        # L'état caché initial h0 est utilisé pour initialiser la sortie de l'unité LSTM pour la première séquence d'entrée, \n",
        "        # et la sortie de la première séquence devient l'état caché pour la deuxième séquence d'entrée, et ainsi de suite.\n",
        "        # La mémoire cellulaire c0 est également utilisée pour stocker des informations importantes sur les séquences précédentes et est passée d'une séquence à l'autre.\n",
        "        h0 = torch.zeros(self.nb_layers, batch_size, self.hidden_dim).to(obs.device)\n",
        "        c0 = torch.zeros(self.nb_layers, batch_size, self.hidden_dim).to(obs.device)\n",
        "        # print(obs.shape, h0.shape,c0.shape)\n",
        "\n",
        "        lstm_out, _ = self.lstm(obs.unsqueeze(0), (h0, c0))  \n",
        "        # lstm_out : batch_size x sequence_length x state_dim\n",
        "        q_values = self.fc(lstm_out[:, -1, :])\n",
        "        self.set((\"q_values\", t), q_values)\n",
        "        if choose_action:\n",
        "            action = q_values.argmax(1)\n",
        "            self.set((\"action\", t), action)\n"
      ],
      "metadata": {
        "id": "E3Ih9vH7QlxO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_name):\n",
        "    return gym.make(env_name)"
      ],
      "metadata": {
        "id": "NPuyITmnD6ar"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EGreedyActionSelector(Agent):\n",
        "    def __init__(self, epsilon):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        q_values = self.get((\"q_values\", t))\n",
        "        nb_actions = q_values.size()[1]\n",
        "        size = q_values.size()[0]\n",
        "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
        "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
        "        max_action = q_values.max(1)[1]\n",
        "        action = is_random * random_action + (1 - is_random) * max_action\n",
        "        action = action.long()\n",
        "        self.set((\"action\", t), action)"
      ],
      "metadata": {
        "id": "MX7ccRhRD7yC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_env_agents(cfg):\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "    get_class(cfg.gym_env),\n",
        "    get_arguments(cfg.gym_env),\n",
        "    cfg.algorithm.nb_evals,\n",
        "    cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent"
      ],
      "metadata": {
        "id": "2st9rCPBD9pY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "    #critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
        "    critic = LSTMQAgent(obs_size, cfg.algorithm.architecture.hidden_size, 2, act_size)\n",
        "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
        "    q_agent = TemporalAgent(critic)\n",
        "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
        "    ev_agent = Agents(eval_env_agent, critic)\n",
        "\n",
        "    # Get an agent that is executed on a complete workspace\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "    return train_agent, eval_agent, q_agent\n"
      ],
      "metadata": {
        "id": "NqikpWm0KK8I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger():\n",
        "\n",
        "  def __init__(self, cfg):\n",
        "    self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "  def add_log(self, log_string, loss, epoch):\n",
        "    self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "  # Log losses\n",
        "  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n",
        "    self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "    self.add_log(\"actor_loss\", a2c_loss, epoch)\n"
      ],
      "metadata": {
        "id": "g-Smn_58EAn_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the optimizer over the q agent\n",
        "def setup_optimizers(cfg, q_agent):\n",
        "    optimizer_args = get_arguments(cfg.optimizer)\n",
        "    parameters = q_agent.parameters()\n",
        "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "8Z9Run9zECAe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, action):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        cfg (_type_): _description_\n",
        "        reward (torch.Tensor): A (T x B) tensor containing the rewards\n",
        "        must_bootstrap (torch.Tensor): a (T x B) tensor containing 0 if the episode is completed at time $t$\n",
        "        q_values (torch.Tensor): a (T x B x A) tensor containing Q values\n",
        "        action (torch.LongTensor): a (T) long tensor containing the chosen action\n",
        "\n",
        "    Returns:\n",
        "        torch.Scalar: The DQN loss and the temporal difference\n",
        "    \"\"\"\n",
        "    # We compute the max of Q-values over all actions\n",
        "    max_q = q_values.max(-1)[0].detach()\n",
        "\n",
        "    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n",
        "    # The same about must_bootstrap. \n",
        "    target = (\n",
        "        reward[:-1] + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
        "    )\n",
        "    # To get Q(s,a), we use torch.gather along the 3rd dimension (the action)\n",
        "    act = action[0].unsqueeze(-1)\n",
        "    qvals = torch.gather(q_values[0], dim=1, index=act).squeeze()\n",
        "\n",
        "    # Compute the temporal difference (use must_boostrap as to mask out finished episodes)\n",
        "    td = target - qvals \n",
        "    # Compute critic loss\n",
        "    td_error = td**2\n",
        "    critic_loss = td_error.mean()\n",
        "    return critic_loss, td\n",
        "    "
      ],
      "metadata": {
        "id": "qcdYS_P6EDna"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn(cfg, max_grad_norm=0.5):\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg)\n",
        "    best_reward = -10e9\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
        "\n",
        "    # 3) Create the DQN-like Agent\n",
        "    train_agent, eval_agent, q_agent = create_dqn_agent(\n",
        "        cfg, train_env_agent, eval_env_agent\n",
        "    )\n",
        "\n",
        "    # 5) Configure the workspace to the right dimension\n",
        "    # Note that no parameter is needed to create the workspace.\n",
        "    # In the training loop, calling the agent() and critic_agent()\n",
        "    # will take the workspace as parameter\n",
        "    train_workspace = Workspace()  # Used for training\n",
        "\n",
        "    # 6) Configure the optimizer over the a2c agent\n",
        "    optimizer = setup_optimizers(cfg, q_agent)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "\n",
        "    # 7) Training loop\n",
        "    for epoch in range(cfg.algorithm.max_epochs):\n",
        "        # print(epoch)\n",
        "        # Execute the agent in the workspace\n",
        "        if epoch > 0:\n",
        "            train_workspace.zero_grad()\n",
        "            train_workspace.copy_n_last_steps(1)\n",
        "            train_agent(\n",
        "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n",
        "            )\n",
        "        else:\n",
        "            train_agent(\n",
        "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
        "            )\n",
        "\n",
        "        transition_workspace = train_workspace.get_transitions()\n",
        "\n",
        "        # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace).\n",
        "        \n",
        "      \n",
        "        q_values, done, truncated, reward, action = transition_workspace[\n",
        "            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
        "        ]\n",
        "\n",
        "        nb_steps += len(action[0]) * cfg.algorithm.n_envs\n",
        "\n",
        "        # Determines whether values of the critic should be propagated\n",
        "        # True if the episode reached a time limit or if the task was not done\n",
        "        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
        "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "        # Compute critic loss\n",
        "        critic_loss, td = compute_critic_loss(\n",
        "            cfg, reward, must_bootstrap, q_values, action\n",
        "        )\n",
        "\n",
        "        # Store the loss for tensorboard display\n",
        "        logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(q_agent.parameters(), max_grad_norm)\n",
        "        obs = train_workspace[\"env/env_obs\"].shape\n",
        "        # print(obs)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "            tmp_steps = nb_steps\n",
        "\n",
        "            eval_workspace = Workspace()  # Used for evaluation\n",
        "\n",
        "            eval_agent(\n",
        "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
        "            )\n",
        "\n",
        "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "\n",
        "            mean = rewards.mean()\n",
        "            logger.add_log(\"reward\", mean, nb_steps)\n",
        "            print(f\"epoch: {epoch}, reward: {mean}\")\n",
        "            if cfg.save_best and mean > best_reward:\n",
        "                best_reward = mean\n",
        "                directory = \"./dqn0_critic/\"\n",
        "                if not os.path.exists(directory):\n",
        "                    os.makedirs(directory)\n",
        "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n",
        "                eval_agent.save_model(filename)\n"
      ],
      "metadata": {
        "id": "ZCbMYucaEF-l"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={\n",
        "  \"save_best\": False,\n",
        "  \"logger\":{\n",
        "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "    \"log_dir\": \"./tmp/\" + str(time.time()),\n",
        "    \"cache_size\": 10000,\n",
        "    \"every_n_seconds\": 10,\n",
        "    \"verbose\": False,    \n",
        "    },\n",
        "\n",
        "  \"algorithm\":{\n",
        "    \"seed\": 5,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"n_envs\": 1,\n",
        "    \"n_steps\": 100,\n",
        "    \"eval_interval\": 2000,\n",
        "    \"nb_evals\": 1,\n",
        "    \"gae\": 0.8,\n",
        "    \"max_epochs\": 3500,\n",
        "    \"discount_factor\": 0.99,\n",
        "    \"batch_size\": 256,\n",
        "    \"max_buffer_size\": 1e5,\n",
        "    \"target_critic_update\": 5000,\n",
        "    #\"architecture\":{\"hidden_size\": [256,256]},\n",
        "    \"architecture\":{\"hidden_size\": 256, \"nb_layers\":2},\n",
        "  },\n",
        "  \"gym_env\":{\n",
        "    \"classname\": \"__main__.make_env\",\n",
        "    \"env_name\": \"CartPole-v1\",\n",
        "  },\n",
        "  \"optimizer\":        # print(batch_size)\n",
        "\n",
        "  {\n",
        "    \"classname\": \"torch.optim.Adam\",\n",
        "    \"lr\": 2e-3,\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "RKSIpUQeEIYd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=OmegaConf.create(params)\n",
        "torch.manual_seed(config.algorithm.seed)\n",
        "run_dqn(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "97XmFJ8NEMMh",
        "outputId": "62e402f8-4495-42fb-823e-aa78e6a377d3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 22, reward: 9.0\n",
            "epoch: 45, reward: 10.0\n",
            "epoch: 68, reward: 9.0\n",
            "epoch: 91, reward: 10.0\n",
            "epoch: 114, reward: 10.0\n",
            "epoch: 137, reward: 10.0\n",
            "epoch: 160, reward: 9.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-776ecb2c75e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-f512bf1afcc8>\u001b[0m in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, max_grad_norm)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_workspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtrain_workspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_n_last_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             train_agent(\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mtrain_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0m_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_variable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bbrl/agents/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mworkspace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[Agent.__call__] workspace must not be None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-9c55db4108a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, choose_action, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# print(obs.shape, h0.shape,c0.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q_values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    775\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'environnement CartPole-v1 est un environnement de simulation où l'objectif est de maintenir un poteau en équilibre en déplaçant un chariot de gauche à droite. L'environnement retourne deux types d'informations : l'observation et l'action.\n",
        "\n",
        "La variable env.observation_space correspond à l'espace des observations de l'environnement. Dans ce cas, c'est un espace continu de 4 dimensions (Box) avec les bornes suivantes : [-4.8, 4.8] pour la position du chariot, [-inf, inf] pour la vitesse du chariot, [-0.42, 0.42] pour l'angle du poteau et [-inf, inf] pour la vitesse angulaire du poteau.\n",
        "\n",
        "La variable env.action_space correspond à l'espace des actions possibles que l'on peut prendre dans l'environnement. Dans ce cas, c'est un espace discret de 2 actions possibles (Discrete) : 0 et 1, qui correspondent à déplacer le chariot à gauche ou à droite, respectivement."
      ],
      "metadata": {
        "id": "dLziUgsl4f40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./tmp"
      ],
      "metadata": {
        "id": "yBTXX7_hEKTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3X19Hz_tVtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}